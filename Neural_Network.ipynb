{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6a2040da-ab4f-46b5-ba71-7a69d7923ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "cc528f1f-8efa-4dbf-8864-e13520257005",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[303]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m weights = [[\u001b[32m0.2\u001b[39m, \u001b[32m0.8\u001b[39m, -\u001b[32m0.5\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m      5\u001b[39m            [\u001b[32m2\u001b[39m, \u001b[32m1.8\u001b[39m, -\u001b[32m0.8\u001b[39m, \u001b[32m1.2\u001b[39m],\n\u001b[32m      6\u001b[39m            [\u001b[32m5\u001b[39m, \u001b[32m0.2\u001b[39m, -\u001b[32m2.5\u001b[39m, \u001b[32m.6\u001b[39m]]\n\u001b[32m      7\u001b[39m biases = [\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m0.5\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m output = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m + bias\n",
      "\u001b[31mValueError\u001b[39m: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "#fully connected neural network, every neuron has a unique connection to every neuron \n",
    "#building a neuron \n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "           [2, 1.8, -0.8, 1.2],\n",
    "           [5, 0.2, -2.5, .6]]\n",
    "biases = [2, 3, 0.5]\n",
    "output = np.dot(weights, input) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a893ae-214c-4847-824e-5561e584158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xip combines two lists in to a list of list, creates a lits by pairing the 0th element of different lists \n",
    " \n",
    "layer_outputs = [] \n",
    "\n",
    "\n",
    "#lol of weight and biases pair\n",
    "for neuron_weights, neuron_bias in zip(weights, biases): #takes pairs of neuron weights and neuron biases \n",
    "    neuron_output = 0 \n",
    "    \n",
    "    for n_input, weight in zip(inputs, neuron_weights): \n",
    "        neuron_output += n_input*weight\n",
    "        \n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "#The shape of array: l = [1,5,6,2] is (4,). 1D array, vector \n",
    "\n",
    "lol = [[1, 5, 6, 2]   #the shape here is (2,4)\n",
    "       [3, 2, 1, 3]] #2d Array,  matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fe7f6-c7ed-4ac4-af53-1c89f172a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a tensor is an objetc that can be represented as an array \n",
    "inputs = [[1, 2, 3, 2.5], \n",
    "          [2, 5, -1, 2], \n",
    "          [-1.5, 2.7, 3.3, -0.8]]  \n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "           [2, 1.8, -0.8, 1.2],\n",
    "           [5, 0.2, -2.5, .6]]\n",
    "biases = [2, 3, 0.5]\n",
    "output = np.dot(weights, inputs) + bias #weights has to be first, indexes accordingly \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43617a7f-f378-4c77-8a90-7e497a562625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling two layers of neurons, move towards object oreinted programming \n",
    "#batches help with generalization\n",
    "#normal batch sizes are 32/64\n",
    "#hidden layers are called hidden layers, as we do not define or decide how the layer changes in values\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(output)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69e760-83d1-4eab-81e1-dc339e6096df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)#initialize the random number generator\n",
    "\n",
    "X = [[1, 2, 3, 2.5], \n",
    "     [2, 5, -1, 2], \n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "\n",
    "class Layer_Dense: \n",
    "    #n_neurons: number of neurons ,n_inputs: number of features. \n",
    "    def __init__(self, n_features , n_neurons): \n",
    "        self.weights = np.random.randn(n_features, n_neurons) #shape is opposite of what was done earlier\n",
    "        \n",
    "        #do so like this as don't need to do transpose later\n",
    "        \n",
    "        self.biases = np.zeros((1, n_neurons)) #(1,n_neurons) is the first parameters not two different parameters \n",
    "        \n",
    "    def forward(self, inputs): \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases #output from previous layer  \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#size of the inputs: How many features for each sample\n",
    "layer1 = Layer_Dense(4, 3) #Output from layer 1 is input 2 layer 2 \n",
    "layer2 = Layer_Dense(3, 6) \n",
    "\n",
    "layer1.forward(X) \n",
    "# print(layer1.output)\n",
    "layer2.forward(layer1.output) \n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955033e6-8577-4d4a-88c9-ba199c8b044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step function:  \n",
    "#Every neuron in the hidden layer as well as the output layer is going to have an activation function associated\n",
    "#geneally, the output layer is going to have a different activation function than what is used in the hidden layers\n",
    "# using sigmoid activation function is easier and more reliabel to train a neural network due to the granularity of the output\n",
    "#ReLu: rectified linear activvation function __/ curve \n",
    "#sigmoid has an issue refered to as the vanishing gradient function \n",
    "\n",
    "\n",
    "inputs = [0,2, -1, 3,.3, -2.7, 1.1, 2.2, -100]\n",
    "output  = []\n",
    "\n",
    "for i in inputs: \n",
    "    if i > 0: \n",
    "        output.append(1)\n",
    "    else: \n",
    "        output.append(0)\n",
    "#Writing a class for the ReLU activation function \n",
    "\n",
    "class Activation_ReLU: \n",
    "    def forward(self, inputs): \n",
    "        self.output = np.maximum(0, inputs) \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612c82f-333e-4bea-970d-65d19653582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a61d6-f398-4540-a676-d673f6cbec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(points, classes):  # how many feature sets over howevery many classes, and two features in side the feature sets (x and y) \n",
    "   #X: denotes feature sets, y: denotes targets or the classifications  \n",
    "    X = np.zeros((points*classes, 2))\n",
    "    y = np.zeros(points*classes, dtype = 'uint8')#8 bit integer\n",
    "\n",
    "    #function that will generate a random dataset for us \n",
    "    for class_number in range(classes): \n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0,1, points) \n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, points)+ np.random.randn(points)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number \n",
    "    return X, y \n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y =create_data(100,3)\n",
    "print(y)\n",
    "plt.scatter(X[:, 0], X[:,1], c=y, s= 40, cmap= plt.cm.Spectral) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "7daed326-ec37-4f12-b656-7e5c2e7a174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to use the data from above for a classification problem\n",
    "\n",
    "np.random.seed(0)#initialize the random number generator\n",
    "\n",
    "class Layer_Dense: \n",
    "    #n_neurons: number of neurons ,n_inputs: number of features. \n",
    "    def __init__(self, n_features , n_neurons): \n",
    "        self.weights = np.random.randn(n_features, n_neurons) #shape is opposite of what was done earlier\n",
    "        \n",
    "        #do so like this as don't need to do transpose later\n",
    "        \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    "        #(1,n_neurons) is the first parameters not two different parameters \n",
    "        \n",
    "    def forward(self, inputs): \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases #output from previous layer  \n",
    "\n",
    "class Activation_ReLU: \n",
    "    def forward(self, inputs): \n",
    "        self.output = np.maximum(0, inputs) \n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs): \n",
    "        exp_values =np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values/ np.sum(exp_values,axis =1, keepdims = True) \n",
    "        self.output = probabilities \n",
    "\n",
    "class Loss: \n",
    "    #ouptut: output from the model, y: intended output values\n",
    "    def calculate(self, output, y): \n",
    "        sample_losses = self.forward(ouput, y) \n",
    "        data_loss = np.mean(sample_losses) \n",
    "        return data_loss\n",
    "        \n",
    "class Loss_CatergoricalCrossentropy(Loss): \n",
    "    def forward(self, y_pred, y_true): \n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, np.exp(-7), 1- np.exp(-7)) \n",
    "        #taking into account one-hot encoded targets and sparse targets\n",
    "        if len(y_true.shape) ==1: \n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true] \n",
    "        elif len(y_true.shape) == 2 : \n",
    "            correct_confidences = np.sum (y_pred* y_true, axis =1) \n",
    "        negative_log_likehlihoods = -log(correct_confidents) \n",
    "        return negative_log_likelihoods  \n",
    "class Accuracy: \n",
    "    def Results(self, predictions, class_targets): \n",
    "        self.results = np.mean(np.array(predictions)  ==np.array(class_targets))\n",
    "        return self.results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac232cf2-064f-42ab-bfa3-5ee8f08e356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of the inputs: How many features for each sample\n",
    "\n",
    "#(2 features, 100 data points) \n",
    "layer1 = Layer_Dense(2, 100)  \n",
    "\n",
    "# take input from the layer and creates a relu for the entrire layer\n",
    "activation1= Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "\n",
    "# softamx actiavtion function is specifically used for the output layer on the classification style neural network problem \n",
    "#Relu us not helpful for the output layer as it always clips all negative numbers as 0, regardless of the magnitude (so impossible to classify if all the nunmbers are negative)\n",
    "\n",
    "\n",
    "#The combination of exponentiation and normalization is called the softmax function\n",
    "\n",
    "layer_outputs = [[4.8, 1.21, 2.385], \n",
    "                 [8.9, -1.81, 0.2],\n",
    "                 [1.41, 1.051, 0.026]]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "norm_values = exp_values/np.sum(exp_values,axis = 1, keepdims= True) \n",
    "#need to set the axis to 1 for row addidition and keepdims to true to make sure the normalization is row-wise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8cf72-1b2c-48fd-b2cd-42716e8eba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data(100, 3)  \n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "e41d263d-e15b-4c04-86de-4599199d1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n",
      "[0.7 0.5 0.9]\n",
      "[0.7 0.5 0.9]\n",
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "#our classification model doesn't output classifcication, it outputs probability distrubition \n",
    "# the general choice of loss function for classification problems using softmax as the activation function \n",
    "#for the output layer is categorical cross-entropy\n",
    "\n",
    "#one-hot encoding: A vector that is n classes long, filled with zeros expect at the index with target class (which would be 1)\n",
    "# When we calculate categorical cross-entropy with one hot vectors, it allows us to simplify it to -log(predicted target class)\n",
    "\n",
    "softmax_outputs= np.array([[0.7, 0.1, 0.2], \n",
    "                   [0.1, 0.5, 0.4], \n",
    "                   [0.02, 0.9, 0.08]])\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs): \n",
    "    print(distribution[targ_idx])\n",
    "\n",
    "# or \n",
    "y1 = softmax_outputs[[0,1,2], class_targets]\n",
    "print(y1) \n",
    "\n",
    "#or \n",
    "y2 = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "print(y2)\n",
    "\n",
    "# now, we find the categotrial cross-entropoy loss \n",
    "\n",
    "loss=(-np.log(y2))\n",
    "print(loss)\n",
    "\n",
    "average_loss = np.mean(loss)\n",
    "print(average_loss)\n",
    "\n",
    "# here, we need to clip the values, to avoid results such as -log(0). We do so in the following way\n",
    "\n",
    "y_clipped= np.clip(y2, np.exp(-7), 1- np.exp(-7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd4d3f-661d-4208-af42-aedf18f19472",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "0e01e558-173b-4414-a9bc-d41c596ce4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs= np.array([[0.7, 0.1, 0.2], \n",
    "                   [0.1, 0.5, 0.4], \n",
    "                   [0.02, 0.9, 0.08]])\n",
    "predictions = np.argmax(softmax_outputs, axis =1) \n",
    "print(predictions) \n",
    "class_targets = [1, 1, 1]\n",
    "print(np.mean(np.array([1, 1, 1]) == np.array([0, 0, 1])) )\n",
    "\n",
    "accuracy = Accuracy() \n",
    "print(accuracy.Results(predictions, class_targets)) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
